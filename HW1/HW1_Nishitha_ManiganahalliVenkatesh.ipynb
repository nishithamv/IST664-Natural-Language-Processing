{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data from json file\n",
    "\n",
    "import json\n",
    "\n",
    "data1 = []\n",
    "for line in open('16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000002.json', 'r'):\n",
    "    data1.append(json.loads(line))\n",
    "    \n",
    "data2 = []\n",
    "for line in open('16119_webhose_2020_02_db21c91a1ab47385bb13773ed8238c31_0000002.json', 'r'):\n",
    "    data2.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the required fields\n",
    "\n",
    "def Merge(dict1, dict2,dict3): \n",
    "    res = {**dict1, **dict2, **dict3} \n",
    "    return res\n",
    "\n",
    "fields1 = [ 'title', 'published', 'author', 'url', 'text']\n",
    "fields2 = ['replies_count', 'country']\n",
    "pt = []\n",
    "\n",
    "def Extract(data):\n",
    "    for rec in data:\n",
    "        lst = {key:value for key,value in rec.items() if key in fields1}\n",
    "        lst1 = {key:value for key,value in rec['thread'].items() if key in fields2}\n",
    "        lst2 = {'facebook':rec['thread']['social']['facebook']}\n",
    "        lst3 = Merge(lst,lst1,lst2)\n",
    "        pt.append(lst3)\n",
    "        \n",
    "Extract(data1)\n",
    "Extract(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing \\n from 'text' field\n",
    "\n",
    "for dic in pt:\n",
    "    text = dic['text']\n",
    "    dic['text'] = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the data into a csv file\n",
    "\n",
    "import csv\n",
    "\n",
    "cols = ['facebook', 'title', 'published', 'replies_count', 'author', 'url', 'country', 'text']\n",
    "with open('test.csv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames = cols)\n",
    "    writer.writeheader()\n",
    "    for dic in pt:\n",
    "        writer.writerow(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data from csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "covid = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing text data to string\n",
    "\n",
    "str1 = ''.join(list(covid['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing using RegexTokenizer by consideringing only alphabets and numbers\n",
    "\n",
    "import nltk\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer('\\w+')\n",
    "r_tokens = tokenizer.tokenize(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing all the tokens to lower case\n",
    "\n",
    "tokens = [word.lower( ) for word in r_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "covid_tokens = [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8adeefdbd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(covid_tokens)\n",
    "fd.plot(30, cumulative = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('said', 343052)\n",
      "('china', 327291)\n",
      "('coronavirus', 301838)\n",
      "('virus', 193760)\n",
      "('people', 178459)\n",
      "('health', 178421)\n",
      "('new', 175110)\n",
      "('1', 143353)\n",
      "('outbreak', 137832)\n",
      "('cases', 133120)\n",
      "('year', 128090)\n",
      "('also', 120448)\n",
      "('2020', 118372)\n",
      "('chinese', 117007)\n",
      "('one', 98708)\n",
      "('wuhan', 91962)\n",
      "('world', 91307)\n",
      "('0', 88367)\n",
      "('two', 82481)\n",
      "('would', 79658)\n",
      "('first', 79299)\n",
      "('2', 79221)\n",
      "('government', 71936)\n",
      "('u', 70285)\n",
      "('last', 69631)\n",
      "('us', 68420)\n",
      "('confirmed', 68323)\n",
      "('3', 67094)\n",
      "('could', 66108)\n",
      "('news', 65892)\n",
      "('time', 65819)\n",
      "('spread', 64832)\n",
      "('million', 62400)\n",
      "('text', 61747)\n",
      "('global', 61337)\n",
      "('public', 61128)\n",
      "('000', 61126)\n",
      "('week', 61048)\n",
      "('reported', 60711)\n",
      "('disease', 59149)\n",
      "('day', 58627)\n",
      "('february', 58450)\n",
      "('feb', 56897)\n",
      "('ship', 56695)\n",
      "('2019', 55762)\n",
      "('5', 55127)\n",
      "('company', 54991)\n",
      "('like', 54876)\n",
      "('10', 53674)\n",
      "('including', 53504)\n"
     ]
    }
   ],
   "source": [
    "#top 50 words by frequency\n",
    "\n",
    "freq_dist = nltk.FreqDist(covid_tokens)\n",
    "most_common_50 = freq_dist.most_common(50)\n",
    "\n",
    "for w in most_common_50:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('of', 'the'), 354918)\n",
      "(('in', 'the'), 254326)\n",
      "(('to', 'the'), 149933)\n",
      "(('the', 'virus'), 126370)\n",
      "(('on', 'the'), 113739)\n",
      "(('for', 'the'), 111931)\n",
      "(('the', 'coronavirus'), 97023)\n",
      "(('in', 'china'), 86148)\n",
      "(('from', 'the'), 84900)\n",
      "(('with', 'the'), 82180)\n",
      "(('and', 'the'), 82076)\n",
      "(('at', 'the'), 81219)\n",
      "(('to', 'be'), 74476)\n",
      "(('have', 'been'), 74318)\n",
      "(('in', 'a'), 73525)\n",
      "(('that', 'the'), 68879)\n",
      "(('more', 'than'), 66486)\n",
      "(('u', 's'), 65138)\n",
      "(('the', 'world'), 65044)\n",
      "(('by', 'the'), 59970)\n",
      "(('the', 'outbreak'), 52364)\n",
      "(('it', 's'), 51748)\n",
      "(('will', 'be'), 51583)\n",
      "(('has', 'been'), 51294)\n",
      "(('china', 's'), 48313)\n",
      "(('said', 'the'), 46911)\n",
      "(('according', 'to'), 46475)\n",
      "(('of', 'a'), 46348)\n",
      "(('number', 'of'), 45618)\n",
      "(('the', 'new'), 44666)\n",
      "(('coronavirus', 'outbreak'), 44558)\n",
      "(('as', 'the'), 42657)\n",
      "(('hong', 'kong'), 42528)\n",
      "(('the', 'first'), 40684)\n",
      "(('he', 'said'), 40560)\n",
      "(('it', 'is'), 39940)\n",
      "(('to', 'a'), 39653)\n",
      "(('novel', 'coronavirus'), 39132)\n",
      "(('due', 'to'), 38710)\n",
      "(('is', 'a'), 37397)\n",
      "(('as', 'a'), 36539)\n",
      "(('the', 'country'), 35637)\n",
      "(('the', 'u'), 35462)\n",
      "(('the', 'chinese'), 35160)\n",
      "(('about', 'the'), 34608)\n",
      "(('covid', '19'), 34453)\n",
      "(('the', 'company'), 33698)\n",
      "(('for', 'a'), 30624)\n",
      "(('with', 'a'), 29616)\n",
      "(('over', 'the'), 29584)\n"
     ]
    }
   ],
   "source": [
    "#top 50 bigrams by frequencies\n",
    "\n",
    "bigrams = list(nltk.bigrams(tokens))\n",
    "bigram_freq_dist = nltk.FreqDist(bigrams)\n",
    "bigram_most_common_50 = bigram_freq_dist.most_common(50)\n",
    "\n",
    "for w in bigram_most_common_50:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('1084673', 'impact_coronavirus_on_the_steel_industry'), 23.454796082453907)\n",
      "(('1087991', 'mmc_industrial_supply'), 23.454796082453907)\n",
      "(('1087993', 'mmc_monitoring'), 23.454796082453907)\n",
      "(('1087995', 'mmc_missions'), 23.454796082453907)\n",
      "(('1deserthottie', 'qtanon1'), 23.454796082453907)\n",
      "(('4346', 'bce3'), 23.454796082453907)\n",
      "(('516bc8cb', 'b44e'), 23.454796082453907)\n",
      "(('5384', '5421'), 23.454796082453907)\n",
      "(('7771', '7805'), 23.454796082453907)\n",
      "(('84883561联系', '凡本网注明'), 23.454796082453907)\n",
      "(('9817', '9858'), 23.454796082453907)\n",
      "(('aff25', 'halston'), 23.454796082453907)\n",
      "(('aline', 'oyamada'), 23.454796082453907)\n",
      "(('andreo', 'calonzo'), 23.454796082453907)\n",
      "(('artha', 'ardhana'), 23.454796082453907)\n",
      "(('asaduddin', 'owaisi'), 23.454796082453907)\n",
      "(('b44e', '4346'), 23.454796082453907)\n",
      "(('babulal', 'marandi'), 23.454796082453907)\n",
      "(('bahçeli̇', 'kktc'), 23.454796082453907)\n",
      "(('bce3', '06590d8e396b'), 23.454796082453907)\n",
      "(('billca', '1deserthottie'), 23.454796082453907)\n",
      "(('bissell', 'linsk'), 23.454796082453907)\n",
      "(('boulardi', 'toti'), 23.454796082453907)\n",
      "(('celestino', 'gallares'), 23.454796082453907)\n",
      "(('chiwuike', 'onyeanu'), 23.454796082453907)\n",
      "(('combizym', 'hirudoid'), 23.454796082453907)\n",
      "(('derhal', 'i̇sti̇fa'), 23.454796082453907)\n",
      "(('ekstra', 'bladet'), 23.454796082453907)\n",
      "(('eren', 'sengezer'), 23.454796082453907)\n",
      "(('ezzeldin', 'bahader'), 23.454796082453907)\n",
      "(('filou', 'oostende'), 23.454796082453907)\n",
      "(('foodgrain', 'prodution'), 23.454796082453907)\n",
      "(('galtung', 'døsvig'), 23.454796082453907)\n",
      "(('ghanti', 'bajao'), 23.454796082453907)\n",
      "(('gonzבlez', 'garcםa'), 23.454796082453907)\n",
      "(('hatidze', 'muratova'), 23.454796082453907)\n",
      "(('heikki', 'kovalainen'), 23.454796082453907)\n",
      "(('intravascular', 'coagulopathy'), 23.454796082453907)\n",
      "(('iodinepowerpack', 'thumbnail_1'), 23.454796082453907)\n",
      "(('i̇sti̇fa', 'etmeli̇'), 23.454796082453907)\n",
      "(('jeoffrey', 'lambujon'), 23.454796082453907)\n",
      "(('jirasat', 'wittaya'), 23.454796082453907)\n",
      "(('joventut', 'badalona'), 23.454796082453907)\n",
      "(('jìnpíng', '习近平'), 23.454796082453907)\n",
      "(('kalagayang', 'epidemiko'), 23.454796082453907)\n",
      "(('kalidou', 'koulibaly'), 23.454796082453907)\n",
      "(('kalyeena', 'makortoff'), 23.454796082453907)\n",
      "(('kimimasa', 'mayama'), 23.454796082453907)\n",
      "(('kkl', 'jnf'), 23.454796082453907)\n",
      "(('kktc', 'cumhurbaşkani'), 23.454796082453907)\n"
     ]
    }
   ],
   "source": [
    "#top 50 bigrams by their mutual information scores (using min frequency 5)\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.BigramCollocationFinder.from_words(tokens)\n",
    "finder.apply_freq_filter(5)\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "for bscore in scored[:50]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106017\n"
     ]
    }
   ],
   "source": [
    "#number of posts\n",
    "\n",
    "posts = len(covid)\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of countries from where data was generated\n",
    "\n",
    "uc = set(covid['country'])\n",
    "len(uc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
